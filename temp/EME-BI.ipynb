{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MFEAcode import *\n",
    "from GNBG import GNBG\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMEBI:\n",
    "    def __init__(self, prob, BASE_POPSZ, BASE_rmp, gen_length, MAX_FES, update_rate, learning, dynamic_pop, save_pop = False):\n",
    "        self.prob = prob\n",
    "        self.base_popsize = 100\n",
    "        self.max_popsize = 100\n",
    "        self.min_popsize = 20\n",
    "        self.inds_tasks = [self.base_popsize] * len(prob)\n",
    "        self.base_rmp = BASE_rmp\n",
    "        self.gen_length = gen_length\n",
    "        self.MAX_FES = MAX_FES\n",
    "        self.delta = None \n",
    "        self.s_rmp = None\n",
    "        self.rmp = np.full((len(prob), len(prob)), BASE_rmp)\n",
    "        self.update_rate = update_rate\n",
    "        self.learningPhase = [LearningPhase(is_start = True, prob = prob) for t in range(len(prob))]\n",
    "        self.learning = learning\n",
    "        self.dynamic_pop = dynamic_pop\n",
    "        self.save_pop = save_pop\n",
    "        self.log = [ [] for _ in range(len(prob))]\n",
    "    def run(self, checkpoint = None):\n",
    "        np.fill_diagonal(self.rmp, 0)\n",
    "        \n",
    "        Parameter.FEl = np.zeros((len(self.prob)))\n",
    "        Parameter.FEs = 0\n",
    "        best = []\n",
    "        if checkpoint is None:\n",
    "            pop = Population(sum(self.inds_tasks), self.gen_length, self.prob)\n",
    "            pop.init(opposition=True)\n",
    "            pop.update_scalar_fitness()\n",
    "            D0, _, _ = pop.calculateD()\n",
    "        if checkpoint is not None:\n",
    "            pop = checkpoint \n",
    "            pop.update_scalar_fitness()\n",
    "            D0, _, _ = pop.calculateD()\n",
    "        this_pop_result = pop.get_result()\n",
    "        for i in range(len(self.prob)):\n",
    "            self.log[i].append([Parameter.FEl[i], this_pop_result[i]])\n",
    "        progress_bar = tqdm(total=self.MAX_FES, unit = 'FEs', unit_scale = True)\n",
    "        while True:\n",
    "            old_fes = Parameter.FEs\n",
    "                 #########################  PHASE 1 - EMEBI ############################\n",
    "            # self.delta = [[[] for _ in range(len(self.prob))] for _ in range(len(self.prob))]\n",
    "            # self.s_rmp = [[[] for _ in range(len(self.prob))] for _ in range(len(self.prob))]\n",
    "            # offsprings = self.reproduction(pop, sum(self.inds_tasks))\n",
    "            # pop.pop.extend(offsprings)\n",
    "            # if self.dynamic_pop:\n",
    "            #     self.inds_tasks = [int(\n",
    "            #         int(max((self.min_popsize - self.max_popsize) * (Parameter.FEs/self.MAX_FES) + self.max_popsize, self.min_popsize))\n",
    "            #     )] * len(self.prob)\n",
    "            # pop.selection_EMEBI(self.inds_tasks)  #each skill-factor choose some best\n",
    "            # # pop.update_scalar_fitness()                 # choose best of the whole pop using scalar-fitness\n",
    "            # # pop.selection(sum(self.inds_tasks))\n",
    "                  \n",
    "            # self.updateRMP(self.update_rate)\n",
    "               #########################  PHASE 1 - MFEA2 ############################\n",
    "            offsprings = pop.reproduction_MFEA_2(sum(self.inds_tasks))\n",
    "            pop.pop.extend(offsprings)\n",
    "            if self.dynamic_pop:\n",
    "                self.inds_tasks = [int(\n",
    "                    int(max((self.min_popsize - self.max_popsize) * (Parameter.FEs/self.MAX_FES) + self.max_popsize, self.min_popsize))\n",
    "                )] * len(self.prob)\n",
    "            pop.selection(sum(self.inds_tasks))\n",
    "            if self.learning:\n",
    "                self.phaseTwo(D0, pop)\n",
    "            new_fes = Parameter.FEs\n",
    "            if Parameter.FEs <= self.MAX_FES:\n",
    "                this_pop_result = pop.get_result()\n",
    "                for i in range(len(self.prob)):\n",
    "                    self.log[i].append([Parameter.FEl[i], this_pop_result[i]])\n",
    "                result = \"Current fitness: \"\n",
    "                for i in range(len(self.prob)):\n",
    "                    result += f\"{i+1}: {this_pop_result[i]} \"\n",
    "                progress_bar.set_description(result)\n",
    "                progress_bar.update(new_fes - old_fes)\n",
    "            else:\n",
    "                break\n",
    "        return self.log, pop\n",
    "            \n",
    "\n",
    "\n",
    "    def reproduction(self, pop, SIZE):\n",
    "        sub_size = int(SIZE/len(self.prob))\n",
    "        offs = []\n",
    "        population = pop.pop\n",
    "        counter = np.zeros((len(self.prob)))\n",
    "        terminateCondition = False\n",
    "        while not terminateCondition:\n",
    "            idx_1, idx_2 = np.random.choice(len(population), 2)\n",
    "            ind1 = population[idx_1]\n",
    "            ind2 = population[idx_2]\n",
    "            t1 = ind1.skill_factor\n",
    "            t2 = ind2.skill_factor\n",
    "            if counter[t1-1] >= sub_size and counter[t2-1] >= sub_size:\n",
    "                continue \n",
    "            rmpValue = np.random.normal(loc = max(self.rmp[t1-1][t2-1], self.rmp[t2-1][t1-1]), scale = 0.1)\n",
    "            if t1 == t2:\n",
    "                off1, off2 = pop.crossover(ind1, ind2)\n",
    "                off1.skill_factor = t1\n",
    "                off2.skill_factor = t2\n",
    "                off1.cal_fitness(self.prob)\n",
    "                off2.cal_fitness(self.prob)\n",
    "                offs.extend([off1, off2])\n",
    "                counter[t1-1] += 2\n",
    "            elif random.random() < rmpValue:\n",
    "                this_offs = pop.crossover(ind1, ind2)\n",
    "                for off in this_offs:\n",
    "                    if counter[t1-1] < sub_size and random.random() < self.rmp[t1-1][t2-1] / (self.rmp[t1-1][t2-1] + self.rmp[t2-1][t1-1]):\n",
    "                        off.skill_factor = t1\n",
    "                        off.cal_fitness(self.prob)\n",
    "                      \n",
    "                        offs.append(off)\n",
    "                        counter[t1-1] += 1\n",
    "                        if ind1.fitness[t1-1] > off.fitness[t1-1]:\n",
    "                            self.delta[t1-1][t2-1].append(ind1.fitness[t1-1] - off.fitness[t1-1])\n",
    "                            self.s_rmp[t1-1][t2-1].append(rmpValue)\n",
    "                    elif counter[t2-1]< sub_size:\n",
    "                        off.skill_factor = t2 \n",
    "                        off.cal_fitness(self.prob)\n",
    "                    \n",
    "                     \n",
    "                        offs.append(off)\n",
    "                        counter[t2-1] +=1\n",
    "                        if ind2.fitness[t2-1] > off.fitness[t2-1]:\n",
    "                            self.delta[t2-1][t1-1].append(ind2.fitness[t2-1] - off.fitness[t2-1])\n",
    "                            self.s_rmp[t2-1][t1-1].append(rmpValue)\n",
    "            else:\n",
    "                if counter[t1 - 1] < sub_size:\n",
    "                            sf_list = np.array( [pop.skill_factor for pop in population] )\n",
    "                           \n",
    "                            idx_same_sf = np.where(sf_list == t1)[0]\n",
    "                   \n",
    "                            random_idx = np.random.choice(idx_same_sf)\n",
    "                            ind11 = population[random_idx]\n",
    "                            assert ind11.skill_factor == t1\n",
    "                            off1, _ = pop.crossover(ind1, ind11)\n",
    "                            off1.skill_factor = t1\n",
    "                            off1.cal_fitness(self.prob)\n",
    "                            offs.append(off1)\n",
    "                            counter[t1-1] += 1\n",
    "                if counter[t2 - 1] < sub_size:\n",
    "                            sf_list = np.array( [pop.skill_factor for pop in population] )\n",
    "                            idx_same_sf = np.where(sf_list == t2)[0]\n",
    "                            random_idx = np.random.choice(idx_same_sf)\n",
    "                            ind22 = population[random_idx]\n",
    "                            assert ind22.skill_factor == t2\n",
    "                            _, off2 = pop.crossover(ind2, ind22)\n",
    "                            off2.skill_factor = t2\n",
    "                            off2.cal_fitness(self.prob)\n",
    "                            offs.append(off2)\n",
    "                            counter[t2-1] += 1\n",
    "            terminateCondition = sum(counter >= sub_size) == len(self.prob)\n",
    "        return offs\n",
    "    def updateRMP(self, update_rate):\n",
    "            for i in range(len(self.prob)):\n",
    "                for j in range(len(self.prob)):\n",
    "                    if i==j:\n",
    "                        continue \n",
    "                    if len(self.delta[i][j]) > 0:\n",
    "                        self.rmp[i][j] += update_rate * Lehmer_mean(self.delta[i][j], self.s_rmp[i][j])\n",
    "                    else:\n",
    "                        self.rmp[i][j] = (1-update_rate)*self.rmp[i][j]\n",
    "                    self.rmp[i][j] = max(0.1, min(1, self.rmp[i][j]))\n",
    "    def phaseTwo(self, D0, pop):\n",
    "        D, maxFit, minFit = pop.calculateD()\n",
    "        maxDelta = maxFit - minFit + 1e-99\n",
    "        sigma = np.where(D > D0 , 0 , 1 - D/D0)\n",
    "        newPop = []\n",
    "        subpops = pop.get_subpops()\n",
    "        for i in range(len(self.prob)):\n",
    "            nextPop = self.learningPhase[i].evolve(subpops[i], sigma[i], maxDelta[i], divisionRate =0.3)\n",
    "            newPop.extend(nextPop)\n",
    "            self.learningPhase[i].start = False\n",
    "        pop.pop = newPop\n",
    "        \n",
    "\n",
    "def Lehmer_mean(delta, s_rmp):\n",
    "        delta = np.array(delta)\n",
    "        s_rmp = np.array(s_rmp)\n",
    "        sum_delta = sum(delta)\n",
    "        tmp = (delta/sum_delta) * s_rmp\n",
    "        meanS = sum(tmp * s_rmp)\n",
    "        return meanS/sum(tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubPop():\n",
    "    def __init__(self, pool, prob, searcher=None):\n",
    "        self.list_tasks = prob\n",
    "        self.pool = pool      #expect a list of Individuals with the same skill_factor, that is self.skill_factor\n",
    "        self.searcher = searcher \n",
    "        self.fitness_improv = 0\n",
    "        self.consume_fes = 0\n",
    "        self.mem_cr = [0.5] * LearningPhase.H\n",
    "        self.mem_f = [0.5] * LearningPhase.H\n",
    "        self.dim = len(pool[0].genes)\n",
    "        self.len_tasks = len(pool[0].fitness)\n",
    "        self.skill_factor = pool[0].skill_factor\n",
    "        # self.task = prob[self.skill_factor - 1]\n",
    "        self.scale = 0.1\n",
    "    def sort_fit(self):\n",
    "        self.pool.sort(key=lambda ind: ind.fitness[self.skill_factor - 1])\n",
    "    def mutation_ind(self, parent):\n",
    "        p = parent.genes \n",
    "        mp = float(1. / p.shape[0])\n",
    "        u = np.random.uniform(size=[p.shape[0]])\n",
    "        r = np.random.uniform(size=[p.shape[0]])\n",
    "        tmp = np.copy(p)\n",
    "        for i in range(p.shape[0]):\n",
    "            if r[i] < mp:\n",
    "                if u[i] < 0.5:\n",
    "                    delta = (2*u[i]) ** (1/(1+Parameter.mum)) - 1\n",
    "                    tmp[i] = p[i] + delta * p[i]\n",
    "                else:\n",
    "                    delta = 1 - (2 * (1 - u[i])) ** (1/(1+Parameter.mum))\n",
    "                    tmp[i] = p[i] + delta * (1 - p[i])\n",
    "        tmp = np.clip(tmp, 0, 1)\n",
    "        ind = Individual(self.dim, self.len_tasks)\n",
    "        ind.genes = tmp\n",
    "        ind.skill_factor = parent.skill_factor\n",
    "        ind.cal_fitness(self.list_tasks)\n",
    "        return ind\n",
    "    def pbest_ind(self, ind, cr, f, best_rate):\n",
    "        assert len(self.pool) > 0\n",
    "        best_size = max (int (len(self.pool) * best_rate), 1)\n",
    "        best = self.pool[:best_size]\n",
    "        pbest = best[random.randint(0, len(best) - 1)]\n",
    "    \n",
    "        rand_idx = np.random.choice(len(self.pool), 2, replace=False)\n",
    "        ind_ran1, ind_ran2 = self.pool[rand_idx[0]], self.pool[rand_idx[1]]\n",
    "        u = (np.random.uniform(0, 1, size=(len(ind.genes)) ) <= cr)\n",
    "        u[np.random.choice(self.dim)] = True\n",
    "\n",
    "        new_genes = np.where(u, \n",
    "            pbest.genes + f * (ind_ran1.genes - ind_ran2.genes),\n",
    "            ind.genes\n",
    "        )\n",
    "        new_genes = np.clip(new_genes, 0, 1)\n",
    "        new_ind = Individual(self.dim, self.len_tasks)\n",
    "        new_ind.genes = new_genes\n",
    "        new_ind.skill_factor = ind.skill_factor\n",
    "        new_ind.cal_fitness(self.list_tasks)\n",
    "        return new_ind\n",
    "    def search(self, mem_cr, mem_f,  sigma, max_delta, best_rate):\n",
    "        s_f = []\n",
    "        s_cr = []\n",
    "        diff_f = []\n",
    "        accumulate_diff = 0\n",
    "        self.sort_fit()\n",
    "        new_pool = []\n",
    "        start_FE = Parameter.FEs\n",
    "        for idx, ind in enumerate(self.pool):\n",
    "            r = random.randint(0, LearningPhase.H - 1)\n",
    "            if(idx==0):\n",
    "                new_pool.append(ind)\n",
    "                continue\n",
    "            print(\"idx=0 still run ???\")\n",
    "            while True:\n",
    "                cr = min(np.random.normal(mem_cr[r], 0.1), 1)\n",
    "                sub_f = np.random.standard_cauchy()\n",
    "                f = min(mem_f[r] + 0.1 * sub_f, 1)\n",
    "                if(cr>=0 and f>=0):\n",
    "                    break\n",
    "            if self.searcher == \"Mutation\":\n",
    "                new_ind = self.mutation_ind(ind)\n",
    "            if self.searcher == \"PBest\":\n",
    "                new_ind = self.pbest_ind(ind, cr, f, best_rate)\n",
    "            diff = ind.fitness[ind.skill_factor-1] - new_ind.fitness[new_ind.skill_factor-1]\n",
    "            \n",
    "            if diff > 0:\n",
    "                accumulate_diff += diff\n",
    "                new_pool.append(new_ind)\n",
    "                if self.searcher == \"PBest\":\n",
    "                    diff_f.append(diff)\n",
    "                    s_cr.append(cr)\n",
    "                    s_f.append(f)\n",
    "            else:\n",
    "                new_pool.append(ind)\n",
    "                # survival_rate = sigma * np.exp(diff/max_delta)\n",
    "                # if random.random() < survival_rate:\n",
    "                #     new_pool.append(new_ind)\n",
    "                # else:\n",
    "                #     new_pool.append(ind)\n",
    "        end_FE = Parameter.FEs\n",
    "        effective = accumulate_diff / (end_FE - start_FE)\n",
    "        assert end_FE - start_FE > 0\n",
    "        assert len(new_pool) == len(self.pool)\n",
    "        if self.searcher == \"PBest\":\n",
    "            return [new_pool, diff_f, s_cr, s_f, effective]\n",
    "        else:\n",
    "            return [new_pool, effective]\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningPhase():\n",
    "    M = 2   #number of operators\n",
    "    H = 10   #F, CR list length\n",
    "    def __init__(self, is_start, prob) -> None:\n",
    "        # self.list_tasks = list_tasks\n",
    "        # self.num_task = len(list_tasks)\n",
    "        # self.dim = dim\n",
    "        self.list_tasks = prob\n",
    "        self.effective = [0] * LearningPhase.M\n",
    "        self.mem_cr = [0.5] * LearningPhase.H\n",
    "        self.mem_f = [0.5] * LearningPhase.H\n",
    "        self.s_cr = []\n",
    "        self.s_f = []\n",
    "        self.diff_f = []\n",
    "        self.mem_pos = 0\n",
    "        self.scale = 0.1\n",
    "        self.start = is_start\n",
    "        self.searcher = [\"PBest\", \"Mutation\"]\n",
    "    def evolve(self, subpop,  sigma: float, max_delta: float, divisionRate: float,) :\n",
    "        # divide subPop wrt divisionRate\n",
    "        new_pool = []\n",
    "        random.shuffle(subpop)\n",
    "        subpop1 = subpop[int(len(subpop)*divisionRate):]   #major\n",
    "        subpop2 = subpop[:int(len(subpop)*divisionRate)]\n",
    "        SubPop1 = SubPop(subpop1, self.list_tasks)\n",
    "        SubPop2 = SubPop(subpop2, self.list_tasks)\n",
    "        opcode = 0\n",
    "        if self.start:\n",
    "            opcode = random.randint(0,1)\n",
    "            SubPop1.searcher = self.searcher[opcode]\n",
    "            SubPop2.searcher = self.searcher[1 - opcode]\n",
    "        else:\n",
    "            self.updateMemory()\n",
    "            opcode = np.argmax(np.array(self.effective))\n",
    "            SubPop1.searcher = self.searcher[opcode]\n",
    "            SubPop2.searcher = self.searcher[1 - opcode]\n",
    "        pool1 = SubPop1.search(self.mem_cr, self.mem_f,  sigma, max_delta, best_rate = 0.1)\n",
    "        pool2 = SubPop2.search(self.mem_cr, self.mem_f,  sigma, max_delta , best_rate = 0.1)\n",
    "        \n",
    "        for pooli in [pool1, pool2]:\n",
    "            if(len(pooli)==2):\n",
    "                new_pool.extend(pooli[0])\n",
    "                self.effective[opcode] = pooli[1]\n",
    "            if(len(pooli)==5):\n",
    "                new_pool.extend(pooli[0])\n",
    "                self.diff_f, self.s_cr, self.s_f , self.effective[1-opcode]= pooli[1], pooli[2], pooli[3], pooli[4]\n",
    "        return new_pool\n",
    "        \n",
    "    # def evolve(self, subPop, sigma: float, max_delta: float) :\n",
    "    #     eval_k = 0\n",
    "    #     pool = []\n",
    "        \n",
    "    #     self.gen += 1\n",
    "    #     if self.gen > 1:\n",
    "    #         self.best_opcode = self.__class__.updateOperator(sum_improve = self.sum_improv, \n",
    "    #                                                          consume_fes = self.consume_fes, \n",
    "    #                                                          M = LearningPhase.M)\n",
    "\n",
    "    #         # self.sum_improv = np.zeros((LearningPhase.M))\n",
    "    #         # self.consume_fes = np.ones((LearningPhase.M))\n",
    "    #         self.sum_improv = [0.0] * LearningPhase.M\n",
    "    #         self.consume_fes = [1.0] * LearningPhase.M\n",
    "\n",
    "        # self.updateMemory()\n",
    "        \n",
    "        # pbest_size = int( 0.1 * len(subPop) )\n",
    "        # subPop.sort(key = lambda ind : ind.fitness[ind.skill_factor-1])\n",
    "        # idx = subPop[:pbest_size]\n",
    "        # pbest = [subPop[i] for i in idx]\n",
    "\n",
    "        # for ind in subPop:\n",
    "        #     r = random.randint(0, LearningPhase.H - 1)\n",
    "        #     cr = np.random.normal(self.mem_cr[r], 0.1)\n",
    "        #     sub_f = np.random.standard_cauchy()\n",
    "        #     f = self.mem_f[r] + 0.1 * sub_f\n",
    "        #     # f = np.random.cauchy(self.mem_f[r], 0.1)\n",
    "                        \n",
    "        #     opcode = random.randint(0, LearningPhase.M)\n",
    "        #     if opcode == LearningPhase.M:\n",
    "        #         opcode = self.best_opcode\n",
    "            \n",
    "        #     if opcode == 0:\n",
    "        #         child = self.searcher[opcode](ind, subPop, pbest, cr, f)\n",
    "        #     elif opcode == 1:\n",
    "        #         child = self.searcher[opcode](ind, return_newInd=True)\n",
    "\n",
    "        #     child.skill_factor = ind.skill_factor\n",
    "        #     child.cal_fitness(self.list_tasks)\n",
    "            \n",
    "            \n",
    "        #     diff = ind.fitness[ind.skill_factor-1] - child.fitness[child.skill_factor-1]\n",
    "        #     if diff > 0:\n",
    "        #         print(\"YAY !\")\n",
    "        #         survival = child\n",
    "\n",
    "        #         self.sum_improv[opcode] += diff\n",
    "\n",
    "        #         if opcode == 0:\n",
    "        #             self.diff_f.append(diff)\n",
    "        #             self.s_cr.append(cr)\n",
    "        #             self.s_f.append(f)\n",
    "                \n",
    "        #     elif diff == 0 or random.random() <= sigma * np.exp(diff/max_delta):\n",
    "        #         survival = child\n",
    "        #     else:\n",
    "        #         survival = ind\n",
    "            \n",
    "        #     pool.append(survival)\n",
    "        \n",
    "        # return pool\n",
    "    \n",
    "\n",
    "\n",
    "    def updateMemory(self):\n",
    "        if len(self.s_cr) > 0:\n",
    "        \n",
    "            self.mem_cr[self.mem_pos] = self.updateMemoryCR(self.diff_f, self.s_cr)\n",
    "            self.mem_f[self.mem_pos] = self.updateMemoryF(self.diff_f, self.s_f)\n",
    "            \n",
    "            self.mem_pos = (self.mem_pos + 1) % LearningPhase.H\n",
    "\n",
    "            self.s_cr = []\n",
    "            self.s_f = []\n",
    "            self.diff_f = []\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def updateMemoryCR(self, diff_f, s_cr) -> float:\n",
    "        diff_f = np.array(diff_f)\n",
    "        s_cr = np.array(s_cr)\n",
    "\n",
    "        sum_diff = sum(diff_f)\n",
    "        weight = diff_f/sum_diff\n",
    "        tmp_sum_cr = sum(weight * s_cr)\n",
    "        return tmp_sum_cr\n",
    "        # mem_cr = sum(weight * s_cr * s_cr)\n",
    "        \n",
    "        # if tmp_sum_cr == 0 or mem_cr == -1:\n",
    "        #     raise ValueError(\"Oh no\")\n",
    "        #     return -1\n",
    "        # else:\n",
    "        #     return mem_cr/tmp_sum_cr\n",
    "        \n",
    "    def updateMemoryF(self, diff_f, s_f) -> float:\n",
    "        diff_f = np.array(diff_f)\n",
    "        s_f = np.array(s_f)\n",
    "\n",
    "        sum_diff = sum(diff_f)\n",
    "        weight = diff_f/sum_diff\n",
    "        tmp_sum_f = sum(weight * s_f)\n",
    "        return sum(weight * (s_f ** 2)) / tmp_sum_f\n",
    "\n",
    "  \n",
    "    # def updateOperator(sum_improve, consume_fes, M: int) -> int:\n",
    "    #     sum_improve = np.array(sum_improve)\n",
    "    #     consume_fes = np.array(consume_fes)\n",
    "    #     eta = sum_improve / consume_fes\n",
    "    #     best_rate = max(eta)\n",
    "    #     best_op = np.argmax(eta)\n",
    "    #     if best_rate > 0:\n",
    "    #         return best_op\n",
    "    #     else:\n",
    "    #         return random.randint(0, M - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "my_list = [2, 3, 4, 5]\n",
    "random.shuffle(my_list)\n",
    "print(my_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Algo = EMEBI(prob= function_list, BASE_POPSZ=100, BASE_rmp=0.3, gen_length=30, MAX_FES=40000, update_rate=0.06, learning=True, dynamic_pop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "class Execute:\n",
    "    def __init__(self, algo, problems, num_run):\n",
    "        self.problems = problems\n",
    "        self.bestValue = [prob.opt_value for prob in problems]\n",
    "        self.algo = algo\n",
    "        self.num_run = num_run\n",
    "        self.attempt = np.zeros((num_run, len(self.algo), len(self.problems)))\n",
    "        self.logs = []\n",
    "    def run(self):\n",
    "        for i in range(self.num_run):\n",
    "            this_algo = copy.deepcopy(self.algo)\n",
    "            tmp_log = []\n",
    "            for idx, algo in enumerate(this_algo):\n",
    "                algo.run()\n",
    "                self.attempt[i][idx] = [self.bestValue[t] - algo.log[t][-1][-1] for t in range(len(self.problems))]\n",
    "                tmp_log.append(algo.log)\n",
    "            self.logs.append(tmp_log)\n",
    "    def convergence_plot(self, Task):\n",
    "        for i in range(len(self.algo)):\n",
    "            LOG = self.logs[0][i][Task - 1]\n",
    "            FEs = [logg[0] for logg in LOG]\n",
    "            fitness = [logg[1] for logg in LOG]\n",
    "            plt.plot(FEs, fitness, label= self.algo[i].name)\n",
    "        # plt.yscale(\"log\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    def statistic(self):\n",
    "        for idx, algo in enumerate(self.algo):\n",
    "            print(self.algo[i].name + \": \" + \"\\n\")\n",
    "            attempt = self.attempt[:, idx, :]\n",
    "            mean_values = attempt.mean(axis=0)\n",
    "            std_values = attempt.std(axis=0)\n",
    "            for col_idx, (mean, std) in enumerate(zip(mean_values, std_values)):\n",
    "                print(f\"Task {col_idx+1}:\")\n",
    "                print(f\"Mean: {mean}\")\n",
    "                print(f\"Std: {std}\")\n",
    "                print(\"-----------------------\")\n",
    "            print(\"----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current fitness: 1: 56548.779524093305 2: -4040.362459224014 :   1%|          | 394/40.0k [00:00<00:19, 2.02kFEs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current fitness: 1: 47278.15821541838 2: -4040.362459224014 :   2%|▏         | 783/40.0k [00:00<00:19, 1.97kFEs/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n",
      "idx=0 still run ???\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m exe \u001b[38;5;241m=\u001b[39m Execute(algo\u001b[38;5;241m=\u001b[39m[Algo], problems\u001b[38;5;241m=\u001b[39mproblems, num_run\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mexe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m, in \u001b[0;36mExecute.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m tmp_log \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, algo \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(this_algo):\n\u001b[1;32m---> 18\u001b[0m     \u001b[43malgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattempt[i][idx] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbestValue[t] \u001b[38;5;241m-\u001b[39m algo\u001b[38;5;241m.\u001b[39mlog[t][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblems))]\n\u001b[0;32m     20\u001b[0m     tmp_log\u001b[38;5;241m.\u001b[39mappend(algo\u001b[38;5;241m.\u001b[39mlog)\n",
      "Cell \u001b[1;32mIn[2], line 44\u001b[0m, in \u001b[0;36mEMEBI.run\u001b[1;34m(self, checkpoint)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta \u001b[38;5;241m=\u001b[39m [[[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob))] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob))]\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_rmp \u001b[38;5;241m=\u001b[39m [[[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob))] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob))]\n\u001b[1;32m---> 44\u001b[0m offsprings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreproduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minds_tasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m pop\u001b[38;5;241m.\u001b[39mpop\u001b[38;5;241m.\u001b[39mextend(offsprings)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_pop:\n",
      "Cell \u001b[1;32mIn[2], line 101\u001b[0m, in \u001b[0;36mEMEBI.reproduction\u001b[1;34m(self, pop, SIZE)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter[t1\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m sub_size \u001b[38;5;129;01mand\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrmp[t1\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][t2\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrmp[t1\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][t2\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrmp[t2\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][t1\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    100\u001b[0m     off\u001b[38;5;241m.\u001b[39mskill_factor \u001b[38;5;241m=\u001b[39m t1\n\u001b[1;32m--> 101\u001b[0m     \u001b[43moff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcal_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     offs\u001b[38;5;241m.\u001b[39mappend(off)\n\u001b[0;32m    104\u001b[0m     counter[t1\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive - Hanoi University of Science and Technology\\Documents\\IHB-Learning\\Numerical Competition\\MFEAcode.py:55\u001b[0m, in \u001b[0;36mIndividual.cal_fitness\u001b[1;34m(self, probs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(probs)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskill_factor:\n\u001b[1;32m---> 55\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfitness[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m         Parameter\u001b[38;5;241m.\u001b[39mFEs \u001b[38;5;241m=\u001b[39m Parameter\u001b[38;5;241m.\u001b[39mFEs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     57\u001b[0m         Parameter\u001b[38;5;241m.\u001b[39mFEl[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m Parameter\u001b[38;5;241m.\u001b[39mFEl[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive - Hanoi University of Science and Technology\\Documents\\IHB-Learning\\Numerical Competition\\GNBG.py:120\u001b[0m, in \u001b[0;36mGNBG.fitness_mfea\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfitness_mfea\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitness_of_ind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLB\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUB\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLB\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive - Hanoi University of Science and Technology\\Documents\\IHB-Learning\\Numerical Competition\\GNBG.py:97\u001b[0m, in \u001b[0;36mGNBG.fitness_of_ind\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_com):\n\u001b[0;32m     96\u001b[0m     inp \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m min_pos[k, :]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mrot_mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43momega\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     b \u001b[38;5;241m=\u001b[39m Transform(rot_mat[:,:,k] \u001b[38;5;241m@\u001b[39m inp, mu[k, :], omega[k, :] )\n\u001b[0;32m     99\u001b[0m     f[\u001b[38;5;241m0\u001b[39m, k] \u001b[38;5;241m=\u001b[39m sigma[k] \u001b[38;5;241m+\u001b[39m (np\u001b[38;5;241m.\u001b[39mabs(a \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(hehe[k, :]) \u001b[38;5;241m@\u001b[39m b))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlamb[k]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive - Hanoi University of Science and Technology\\Documents\\IHB-Learning\\Numerical Competition\\GNBG.py:123\u001b[0m, in \u001b[0;36mTransform\u001b[1;34m(X, Alpha, Beta)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTransform\u001b[39m(X, Alpha, Beta):\n\u001b[1;32m--> 123\u001b[0m     Y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    124\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m (X \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    125\u001b[0m     Y[tmp] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(X[tmp])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current fitness: 1: 47278.15821541838 2: -4040.362459224014 :   2%|▏         | 783/40.0k [00:20<00:19, 1.97kFEs/s]"
     ]
    }
   ],
   "source": [
    "exe = Execute(algo=[Algo], problems=problems, num_run=2)\n",
    "exe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm 1: \n",
      "\n",
      "Column 1:\n",
      "Mean: -682.1000910462726\n",
      "Std: 3.837864460365381e-05\n",
      "-----------------------\n",
      "Column 2:\n",
      "Mean: 0.0\n",
      "Std: 0.0\n",
      "-----------------------\n",
      "----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exe.statistic()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
